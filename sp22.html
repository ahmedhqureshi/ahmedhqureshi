<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Purdue CS 59300 Robotics (Spring 2022)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="Curriculum_Vitae.pdf">CV</a></div>
<div class="menu-category">Links</div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=Lkrx2SkAAAAJ&hl=en">Google&nbsp;Scholar</a></div>
<div class="menu-item"><a href="https://docs.google.com/forms/d/e/1FAIpQLSeq_yN58oOFeWIsxrVfBVMDCkjhBoiHNFs0MeHBe_lW356-Hg/viewform">Join&nbsp;US</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Purdue CS 59300 Robotics (Spring 2022)</h1>
</div>
<p>This course covers topics in motion planning, estimation, and control to design algorithms for robots to safely interact with their environments and perform various challenging tasks under constraints. The first part of this course focuses on classical techniques such as search-based and sampling-based planning, PID control, linear quadratic regulation (LQR), and Bayesian filters. The second part covers modern deep learning and deep reinforcement learning techniques and their application to planning and decision-making in robotics. </p>
<h2>Course Content </h2>
<table id="course">
<tr class="r1"><td class="c1"><b>Sr</b> </td><td class="c2"> <b>Date</b> </td><td class="c3"> <b>Topics (Tentative)</b> </td><td class="c4"> <b>Readings</b> </td><td class="c5"> <b>Assignments</b> </td></tr>
<tr class="r2"><td class="c1">1 </td><td class="c2"> 1/11 </td><td class="c3"> Introduction to Robotics </td><td class="c4"> </td></tr>
<tr class="r3"><td class="c1">2 </td><td class="c2">1/13 </td><td class="c3"> Robot Manipulators (Forward/Inverse Kinematics) </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r4"><td class="c1">3 </td><td class="c2">1/18 </td><td class="c3"> Planning I: Search-based approaches (A*, D*) </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r5"><td class="c1">4 </td><td class="c2">1/20 </td><td class="c3"> Planning II: Sampling-based methods I (RRT, PRM) </td><td class="c4"> </td><td class="c5"> Assignment 1 </td></tr>
<tr class="r6"><td class="c1">5 </td><td class="c2">1/25 </td><td class="c3"> Planning III: Sampling-based methods II (RRTConnect, RRT*, B-RRT*) </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r7"><td class="c1">6 </td><td class="c2">1/27 </td><td class="c3"> Optimal Control I: (PID, LQR) </td><td class="c4"> </td></tr>
<tr class="r8"><td class="c1">7 </td><td class="c2">2/1  </td><td class="c3"> Optimal Control II: (Collocation, Shooting, MPC) </td><td class="c4"> </td><td class="c5">  </td></tr>
<tr class="r9"><td class="c1">8 </td><td class="c2">2/3  </td><td class="c3"> Estimation I: Probability Review, Bayes Filtering </td><td class="c4"> </td><td class="c5">  </td></tr>
<tr class="r10"><td class="c1">9 </td><td class="c2">2/8  </td><td class="c3"> Estimation II: Kalman Filter </td><td class="c4"> </td><td class="c5"> Assignment 2 </td></tr>
<tr class="r11"><td class="c1">10 </td><td class="c2">2/10 </td><td class="c3"> Planning, Estimation &amp; Control: LQR-RRT, LQG-MP </td><td class="c4"> </td><td class="c5">  </td></tr>
<tr class="r12"><td class="c1">11 </td><td class="c2">2/15 </td><td class="c3"> Function Approximation I: Neural Networks, Backpropagation, Dropout </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r13"><td class="c1">12 </td><td class="c2">2/17 </td><td class="c3"> Function Approximation II: Convolutional Neural Networks, Recurrent Neural Networks (LSTMs) </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r14"><td class="c1">13 </td><td class="c2">2/22 </td><td class="c3"> Function Approximation III: Variational Autoencoders (CVAEs), Generative Adversarial Networks </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r15"><td class="c1">14 </td><td class="c2">2/24 </td><td class="c3"> Learning for Planning I: Informed Sampling </td><td class="c4"> </td><td class="c5"> Assignment 3 </td></tr>
<tr class="r16"><td class="c1">15 </td><td class="c2">3/1  </td><td class="c3"> Learning for Planning II: Informed Path Generation </td><td class="c4"> </td><td class="c5">  </td></tr>
<tr class="r17"><td class="c1">16 </td><td class="c2">3/3  </td><td class="c3"> Deep Reinforcement Learning I: MDPs, Value Function, Q-Function </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r18"><td class="c1">17 </td><td class="c2">3/8  </td><td class="c3"> Deep Reinforcement Learning II: Policy Gradients (Reinforce) </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r19"><td class="c1">18 </td><td class="c2">3/10 </td><td class="c3"> Deep Reinforcement Learning III: Advance Policy Gradients Methods I  </td><td class="c4"> </td><td class="c5">  </td></tr>
<tr class="r20"><td class="c1">19 </td><td class="c2">3/15 </td><td class="c3"> Deep Reinforcement Learning IV: Advance Policy Gradients Methods II  </td><td class="c4">  </td><td class="c5"> Assignment 4 </td></tr>
<tr class="r21"><td class="c1">20 </td><td class="c2">3/17 </td><td class="c3"> Deep Reinforcement Learning V: Model-based Reinforcement Learning </td><td class="c4"> </td><td class="c5">  </td></tr>
<tr class="r22"><td class="c1">21 </td><td class="c2">3/22 </td><td class="c3"> Imitation Learning I: DAGGAR  </td><td class="c4"> </td><td class="c5"> Project Proposals (Due)  </td></tr>
<tr class="r23"><td class="c1">22 </td><td class="c2">3/24 </td><td class="c3"> Imitation Learning II: GAIL, InfoGAIL  </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r24"><td class="c1">23 </td><td class="c2">3/29 </td><td class="c3"> Inverse Reinforcement Learning I: Maximum Entropy(MaxEnt)-IR</td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r25"><td class="c1">24 </td><td class="c2">3/31 </td><td class="c3"> Inverse Reinforcement Learning II: GANs &amp; IRL</td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r26"><td class="c1">25 </td><td class="c2">4/5  </td><td class="c3"> Deep Reinforcement Learning for Planning I </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r27"><td class="c1">26 </td><td class="c2">4/7  </td><td class="c3"> Deep Reinforcement Learning for Planning II </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r28"><td class="c1">27 </td><td class="c2">4/12 </td><td class="c3"> Closing Remarks, Discussions &amp; Open Research Problems </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r29"><td class="c1">28 </td><td class="c2">4/14 </td><td class="c3"> Project Presentations I </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r30"><td class="c1">29 </td><td class="c2">4/19 </td><td class="c3"> Project Presentations II </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r31"><td class="c1">30 </td><td class="c2">4/21 </td><td class="c3"> Project Presentations III </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r32"><td class="c1">31 </td><td class="c2">4/26 </td><td class="c3"> Project Presentations IV </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r33"><td class="c1">32 </td><td class="c2">4/28 </td><td class="c3"> Project Presentations V </td><td class="c4"> </td><td class="c5"> </td></tr>
<tr class="r34"><td class="c1">
</td></tr></table>
<h2>Prerequisites</h2>
<p>Familiarity with Data Structures &amp; Algorithms, and a background in AI &amp; Statistical Estimation is required. </p>
<h2>Textbooks</h2>
<p>There are no specific textbooks. However, some helpful reference books include: </p>
<ul>
<li><p>Principles of Robot Motion (Theory, Algorithms, and Implementations) by Howie Choset, Kevin M. Lynch, Seth Hutchinson, George A. Kantor, Wolfram Burgard, Lydia E. Kavraki and Sebastian Thrun. </p>
</li>
<li><p>Planning Algorithms by Steven M LaValle. Cambridge university press, 2006. </p>
</li>
<li><p>Probabilistic Robotics by Wolfram Burgard, Dieter Fox, and Sebastian Thrun. MIT Press (2005).</p>
</li>
<li><p>Reinforcement learning: An introduction by Richard S. Sutton, and Andrew G. Barto. MIT press, 2018.</p>
</li>
<li><p>Deep Learning by Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. MIT press, 2016.</p>
</li>
</ul>
<h2>Grading Policy</h2>
<p>There are no midterm and final exams. </p>
<table id="Grading">
<tr class="r1"><td class="c1">Assignments (4 X 10%) </td><td class="c2"> 40% </td></tr>
<tr class="r2"><td class="c1">Project proposal </td><td class="c2"> 10% </td></tr>
<tr class="r3"><td class="c1">Project Completion & Demo </td><td class="c2"> 30% </td></tr>
<tr class="r4"><td class="c1">Project Report &amp; Presentation </td><td class="c2"> 15% </td></tr>
<tr class="r5"><td class="c1">Class Participation </td><td class="c2"> 5% </td></tr>
<tr class="r6"><td class="c1">
</td></tr></table>
<h2>Administrative Information</h2>
<ul>
<li><p>Instructor: Ahmed H. Qureshi (ahqureshi@purdue.edu)</p>
</li>
<li><p>TAs: TBD</p>
</li>
<li><p>Time &amp; Location: TBD</p>
</li>
</ul>
<h2>Academic Integrity</h2>
<p>This course defaults to Purdue standards on intellectual integrity and academic conduct. Therefore, students are responsible for reading the following pages and comply with them throughout this course.</p>
<ul>
<li><p><a href="https://spaf.cerias.purdue.edu/integrity.html">https://spaf.cerias.purdue.edu/integrity.html</a></p>
</li>
<li><p><a href="https://spaf.cerias.purdue.edu/cpolicy.html">https://spaf.cerias.purdue.edu/cpolicy.html</a></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-09-29 14:56:51 US Eastern Daylight Time, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
